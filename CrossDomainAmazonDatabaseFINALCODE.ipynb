{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CrossDomainAmazonDatabase.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPqd7Vf2p3kp",
        "colab_type": "text"
      },
      "source": [
        "### Projeto de Mineração de texto - Cross Domain com bases da Amazon\n",
        "Grupo : Emilia Galdino e Leonardo Monte\n",
        "\n",
        "\n",
        "Universidade Federal Rural de Pernambuco \n",
        "\n",
        "\n",
        "Bacharelado em Ciência da Computação\n",
        "\n",
        "\n",
        "Mineração de Texto - 2019.2\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHDLLna6DKHM",
        "colab_type": "code",
        "outputId": "793cf591-c14b-41c7-ae3b-bdef38a42cca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        }
      },
      "source": [
        "!git clone https://github.com/LeonardoMonte/CrossDomainMineracao2019.2.git\n",
        "path = 'CrossDomainMineracao2019.2/database/'\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.utils import shuffle\n",
        "import pickle\n",
        "import spacy\n",
        "import unicodedata\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "from multiprocessing import Pool\n",
        "from tqdm import tqdm\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import pickle\n",
        "from collections import defaultdict\n",
        "from scipy.stats import friedmanchisquare\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "import time\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import cv2\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "import nltk\n",
        "import string\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "import os\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'CrossDomainMineracao2019.2'...\n",
            "remote: Enumerating objects: 53, done.\u001b[K\n",
            "remote: Counting objects:   1% (1/53)\u001b[K\rremote: Counting objects:   3% (2/53)\u001b[K\rremote: Counting objects:   5% (3/53)\u001b[K\rremote: Counting objects:   7% (4/53)\u001b[K\rremote: Counting objects:   9% (5/53)\u001b[K\rremote: Counting objects:  11% (6/53)\u001b[K\rremote: Counting objects:  13% (7/53)\u001b[K\rremote: Counting objects:  15% (8/53)\u001b[K\rremote: Counting objects:  16% (9/53)\u001b[K\rremote: Counting objects:  18% (10/53)\u001b[K\rremote: Counting objects:  20% (11/53)\u001b[K\rremote: Counting objects:  22% (12/53)\u001b[K\rremote: Counting objects:  24% (13/53)\u001b[K\rremote: Counting objects:  26% (14/53)\u001b[K\rremote: Counting objects:  28% (15/53)\u001b[K\rremote: Counting objects:  30% (16/53)\u001b[K\rremote: Counting objects:  32% (17/53)\u001b[K\rremote: Counting objects:  33% (18/53)\u001b[K\rremote: Counting objects:  35% (19/53)\u001b[K\rremote: Counting objects:  37% (20/53)\u001b[K\rremote: Counting objects:  39% (21/53)\u001b[K\rremote: Counting objects:  41% (22/53)\u001b[K\rremote: Counting objects:  43% (23/53)\u001b[K\rremote: Counting objects:  45% (24/53)\u001b[K\rremote: Counting objects:  47% (25/53)\u001b[K\rremote: Counting objects:  49% (26/53)\u001b[K\rremote: Counting objects:  50% (27/53)\u001b[K\rremote: Counting objects:  52% (28/53)\u001b[K\rremote: Counting objects:  54% (29/53)\u001b[K\rremote: Counting objects:  56% (30/53)\u001b[K\rremote: Counting objects:  58% (31/53)\u001b[K\rremote: Counting objects:  60% (32/53)\u001b[K\rremote: Counting objects:  62% (33/53)\u001b[K\rremote: Counting objects:  64% (34/53)\u001b[K\rremote: Counting objects:  66% (35/53)\u001b[K\rremote: Counting objects:  67% (36/53)\u001b[K\rremote: Counting objects:  69% (37/53)\u001b[K\rremote: Counting objects:  71% (38/53)\u001b[K\rremote: Counting objects:  73% (39/53)\u001b[K\rremote: Counting objects:  75% (40/53)\u001b[K\rremote: Counting objects:  77% (41/53)\u001b[K\rremote: Counting objects:  79% (42/53)\u001b[K\rremote: Counting objects:  81% (43/53)\u001b[K\rremote: Counting objects:  83% (44/53)\u001b[K\rremote: Counting objects:  84% (45/53)\u001b[K\rremote: Counting objects:  86% (46/53)\u001b[K\rremote: Counting objects:  88% (47/53)\u001b[K\rremote: Counting objects:  90% (48/53)\u001b[K\rremote: Counting objects:  92% (49/53)\u001b[K\rremote: Counting objects:  94% (50/53)\u001b[K\rremote: Counting objects:  96% (51/53)\u001b[K\rremote: Counting objects:  98% (52/53)\u001b[K\rremote: Counting objects: 100% (53/53)\u001b[K\rremote: Counting objects: 100% (53/53), done.\u001b[K\n",
            "remote: Compressing objects: 100% (47/47), done.\u001b[K\n",
            "remote: Total 53 (delta 5), reused 41 (delta 2), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (53/53), done.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1Xz9EUuA7K_",
        "colab_type": "text"
      },
      "source": [
        "## Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovyi9_PlA8gz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read(address):\n",
        "    arq = open(address, 'rb')\n",
        "    return pickle.load(arq)\n",
        "\n",
        "\n",
        "def save_pickle(address, element):\n",
        "    arq = open(address, 'wb')\n",
        "    pickle.dump(element, arq)\n",
        "\n",
        "\n",
        "def save_data_frame(s, name):\n",
        "    s.to_csv(name + '.csv', index=False)\n",
        "\n",
        "### MUDAR FONTE\n",
        "def return_domain(domain_name):\n",
        "    return read(\"drive/My Drive/sa/DATASET/\"+domain_name+\".pk\")\n",
        "\n",
        "def remove_accented_chars(text):\n",
        "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "    return text\n",
        "\n",
        "def remove_special_characters(text, remove_digits=False):\n",
        "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
        "    text = re.sub(pattern, '', text)\n",
        "    return text\n",
        "\n",
        "neg_words = ['not', 'no', \"n't\", \"never\", \"nothing\"]\n",
        "def tokenize(doc):\n",
        "    stop = stopwords.words('english')\n",
        "    x = []\n",
        "    t = word_tokenize(doc)\n",
        "    for a in t:\n",
        "        a = a.lower()\n",
        "        if a in \"n't\":\n",
        "                x.append(\"not\")\n",
        "        elif a not in stop or a in neg_words:\n",
        "            if a.isalpha() and len(a) > 1:\n",
        "                x.append(WordNetLemmatizer().lemmatize(a))\n",
        "    return x\n",
        "  \n",
        "\n",
        "# The input is a tokenize document\n",
        "pos_import = [\"NN\", \"VB\", \"JJ\", \"RB\"]\n",
        "\n",
        "\n",
        "def pos_filter(token_doc,nword):\n",
        "    pos = nltk.pos_tag(token_doc)\n",
        "    new_t = []\n",
        "    jump = -1\n",
        "    for p in pos:\n",
        "        k = [True if po in p[1] else False for po in pos_import]\n",
        "        if any(k) or ((p[0] in neg_words)):\n",
        "            if p[0] in neg_words and nword:\n",
        "                n, jump = neg_affect(p, pos, 4)\n",
        "                if n != \"\":\n",
        "                    new_t.append(n)\n",
        "            else:\n",
        "                if pos.index(p) == jump:\n",
        "                    pass\n",
        "                else:\n",
        "                    new_t.append(p[0])\n",
        "\n",
        "    return new_t\n",
        "  \n",
        "# w is window\n",
        "def neg_affect(word, pos, w):\n",
        "    neg_pos = [\"VB\", \"JJ\", \"RB\"]\n",
        "    i = pos.index(word)\n",
        "    if word[0] == \"no\":\n",
        "        try:\n",
        "            if \"NN\" in pos[i + 1][1]:\n",
        "                n = pos[i + 1][0] + \"_NOT\"\n",
        "                return n, i + 1\n",
        "        except:\n",
        "            return \"\", -1\n",
        "    else:\n",
        "        for j in range(i + 1, i + w):\n",
        "            try:\n",
        "                k = [True if po in pos[j][1] else False for po in neg_pos]\n",
        "                if any(k):\n",
        "                    n = pos[j][0] + \"_NOT\"\n",
        "                    return n, j\n",
        "                elif pos[j][1] in punct:\n",
        "                    return \"\", -1\n",
        "            except:\n",
        "                return \"\", -1\n",
        "    return \"\", -1\n",
        "\n",
        "def n_gr(token_doc):\n",
        "    new_token = []\n",
        "    for i in range(len(token_doc)):\n",
        "        if i < (len(token_doc) - 1):\n",
        "            new_token.append(token_doc[i])\n",
        "            bi_gram = \"%s__%s\" % (token_doc[i], token_doc[i + 1])\n",
        "            new_token.append(bi_gram)\n",
        "    return new_token\n",
        "\n",
        "\n",
        "# Tokenize + lemmatize + remove stop\n",
        "# Pos filter + negation words\n",
        "# bigram\n",
        "\n",
        "def pipeline(doc):\n",
        "    t = tokenize(doc)\n",
        "    nt = pos_filter(t,True)\n",
        "    nt_gram = n_gram(nt)\n",
        "    return nt_gram\n",
        "\n",
        "  \n",
        "\n",
        "def setofwords(datapipeline):\n",
        "  words = []\n",
        "  for doc in datapipeline:\n",
        "    for word in doc:\n",
        "      words.append(word)\n",
        "\n",
        "  words = list(set(words))\n",
        "\n",
        "  return words\n",
        "\n",
        "\n",
        "\n",
        "def word_frequency(all_doc):\n",
        "  dic = {}\n",
        "  vocab = []\n",
        " \n",
        "  for token in all_doc:\n",
        "    for w in token:\n",
        "      if w in vocab:\n",
        "        dic[w] += 1\n",
        "      else:\n",
        "        vocab.append(w)\n",
        "        dic[w] = 1\n",
        "  return dic\n",
        "\n",
        "def bow_bin(token_rev,vocab):\n",
        "  bag = []\n",
        "  bag.append(vocab)\n",
        "  doc = sorted(set(vocab))\n",
        "  for doc in token_rev:\n",
        "    val = np.zeros(len(vocab))\n",
        "    for w in doc:\n",
        "      try:\n",
        "        i = vocab.index(w)\n",
        "        val[i] = 1\n",
        "      except:\n",
        "        pass\n",
        "    bag.append(val)\n",
        "  return bag\n",
        "\n",
        "def word_frequency(all_doc):\n",
        "  dic = {}\n",
        "  vocab = []\n",
        " \n",
        "  for token in all_doc:\n",
        "    for w in token:\n",
        "      if w in vocab:\n",
        "        dic[w] += 1\n",
        "      else:\n",
        "        vocab.append(w)\n",
        "        dic[w] = 1\n",
        "  return dic\n",
        "\n",
        "def thr_lim(all_doc,thr):\n",
        "  dic = word_frequency(all_doc)\n",
        "  new_rev = []\n",
        "  for i in list(dic):\n",
        "    if dic[i] < thr:\n",
        "      del(dic[i])\n",
        "  for doc in all_doc:\n",
        "    new = []\n",
        "    for w in doc:\n",
        "      try:\n",
        "        dic[w]\n",
        "        new.append(w)\n",
        "      except:\n",
        "        pass\n",
        "    new_rev.append(new)\n",
        "    \n",
        "  return new_rev,dic\n",
        "\n",
        "def amostra_estrat(tam , df , classe):\n",
        "\n",
        "    classes = df[classe].unique()\n",
        "    qtde_por_classe = round(tam / len(classes))\n",
        "    amostras_por_classe = []\n",
        "\n",
        "    for c in classes:\n",
        "        indices_c = df[classe] == c\n",
        "        obs_c = df[indices_c]\n",
        "        amostra_c = obs_c.sample(qtde_por_classe)\n",
        "        amostras_por_classe.append(amostra_c)\n",
        "    amostra_estratificada = pd.concat(amostras_por_classe)\n",
        "\n",
        "    return amostra_estratificada\n",
        "\n",
        "def preprocess(token_rev, nword, pos, ng, thr):\n",
        "  prepData = []\n",
        "  \n",
        "  for doc in (token_rev):\n",
        "    prepData.append(selecPipe(doc, nword, pos, ng))\n",
        "  if thr > 0:\n",
        "    prepData, wordFreq = thr_lim(prepData,thr)\n",
        "  else:\n",
        "    wordFreq = word_frequency(prepData)\n",
        "  return prepData, wordFreq\n",
        "\n",
        "def selecPipe(doc, nword, pos, ng):\n",
        "  t = tokenize(doc)\n",
        "  if pos == True:\n",
        "    t = pos_filter(t,nword) \n",
        "  if ng == True:\n",
        "    t = n_gr(t)\n",
        "  return t \n",
        "\n",
        "##### LOAD #####\n",
        "\n",
        "# loading the datasets from the Git repositiry\n",
        "def loadpreprocess(path,estra,tr,nword = True, pos = True, ng = True):\n",
        "\n",
        "  books = read(path+'books.pk')\n",
        "  dvd = read(path+'dvd.pk')\n",
        "  electronics = read(path+'electronics.pk')\n",
        "  kitchen = read(path+'kitchen.pk')\n",
        "\n",
        "  frames1 = [pd.DataFrame(books[0]),pd.DataFrame(books[1])]\n",
        "  booksnew = pd.concat(frames1,axis = 1, ignore_index=True)\n",
        "\n",
        "  frames2 = [pd.DataFrame(dvd[0]),pd.DataFrame(dvd[1])]\n",
        "  dvdnew = pd.concat(frames2,axis = 1, ignore_index=True)\n",
        "\n",
        "  frames3 = [pd.DataFrame(electronics[0]),pd.DataFrame(electronics[1])]\n",
        "  electronicsnew = pd.concat(frames3,axis = 1, ignore_index=True)\n",
        "\n",
        "  frames4 = [pd.DataFrame(kitchen[0]),pd.DataFrame(kitchen[1])]\n",
        "  kitchennew = pd.concat(frames4,axis = 1, ignore_index=True)\n",
        "\n",
        "\n",
        "  booksnew = amostra_estrat(estra , booksnew , 1)\n",
        "  dvdnew = amostra_estrat(estra , dvdnew , 1)\n",
        "  electronicsnew = amostra_estrat(estra , electronicsnew , 1)\n",
        "  kitchennew = amostra_estrat(estra , kitchennew , 1)\n",
        "\n",
        "  books[0],books[1] = list(booksnew[0]),list(booksnew[1])\n",
        "  dvd[0],dvd[1] = list(dvdnew[0]),list(dvdnew[1])\n",
        "  electronics[0],electronics[1] = list(electronicsnew[0]),list(electronicsnew[1])\n",
        "  kitchen[0],kitchen[1] = list(kitchennew[0]),list(kitchennew[1])\n",
        "\n",
        "\n",
        "  #### END LOAD ####\n",
        "\n",
        "  ##### PIPILINE PRE PROCESING ###\n",
        "\n",
        "  bookspipeline,_ = preprocess(books[0], nword, pos, ng, tr)\n",
        "  dvdpipeline,_ = preprocess(dvd[0], nword, pos, ng, tr)\n",
        "  electronicspipeline,_ = preprocess(electronics[0], nword, pos, ng, tr)\n",
        "  kitchenpipeline,_ = preprocess(kitchen[0], nword, pos, ng, tr)\n",
        "\n",
        "  booksbow = np.array(bow_bin(bookspipeline,setofwords(bookspipeline)))\n",
        "  dvdbow = np.array(bow_bin(dvdpipeline,setofwords(dvdpipeline)))\n",
        "  electronicsbow = np.array(bow_bin(electronicspipeline,setofwords(electronicspipeline)))\n",
        "  kitchenbow = np.array(bow_bin(kitchenpipeline,setofwords(kitchenpipeline)))\n",
        "\n",
        "  books[0],books[1] = booksbow,np.array(books[1])\n",
        "  dvd[0],dvd[1] = dvdbow,np.array(dvd[1])\n",
        "  electronics[0],electronics[1] = electronicsbow,np.array(electronics[1])\n",
        "  kitchen[0],kitchen[1] = kitchenbow,np.array(kitchen[1])\n",
        "\n",
        "  books = [books[0][1:].astype(np.float),books[1].astype(np.float)]\n",
        "  dvd = [dvd[0][1:].astype(np.float),dvd[1].astype(np.float)]\n",
        "  electronics = [electronics[0][1:].astype(np.float),electronics[1].astype(np.float)]\n",
        "  kitchen = [kitchen[0][1:].astype(np.float),kitchen[1].astype(np.float)]\n",
        "\n",
        "  return books,dvd,electronics,kitchen\n",
        "\n",
        "\n",
        "def kfoldexperiment(kvalue,kseed,ite,bd,bdclass,databasename, cmatrix = True):\n",
        "\n",
        "  kfold = KFold(kvalue, True, kseed)\n",
        "\n",
        "  naive = GaussianNB()\n",
        "  svmrbf = SVC(kernel='rbf' , gamma='scale')\n",
        "  logRegre = LogisticRegression(solver='lbfgs')\n",
        "\n",
        "  if cmatrix:\n",
        "    svmmatrix = []\n",
        "    logmatrix = []\n",
        "    naivematrix = []\n",
        "\n",
        "  svmrbfarray = []\n",
        "  logRegrearray = []\n",
        "  naivearray = []\n",
        "\n",
        "  svmrbftime = []\n",
        "  logRegretime = []\n",
        "  naivetime = []\n",
        "\n",
        "  for x in range(ite):\n",
        "\n",
        "    c = kfold.split(bd)\n",
        "\n",
        "    for train_index, test_index in c:\n",
        "\n",
        "      noclass_train, noclass_test = bd[train_index], bd[test_index]\n",
        "      class_train, class_test = bdclass[train_index], bdclass[test_index]\n",
        "\n",
        "      naivestart = time.time()\n",
        "      naive.fit(noclass_train,class_train)\n",
        "      naivearray.append(naive.score(noclass_test, class_test))\n",
        "\n",
        "      if cmatrix:\n",
        "        naivematrix.append(confusion_matrix(class_test, naive.predict(noclass_test)))\n",
        "\n",
        "      naiveend = time.time()\n",
        "      naivetime.append(naiveend - naivestart)\n",
        "\n",
        "      logRegrestart = time.time()\n",
        "      logRegre.fit(noclass_train,class_train)\n",
        "      logRegrearray.append(logRegre.score(noclass_test,class_test))\n",
        "\n",
        "      if cmatrix:\n",
        "        logmatrix.append(confusion_matrix(class_test, logRegre.predict(noclass_test)))\n",
        "\n",
        "      logRegreend = time.time()\n",
        "      logRegretime.append(logRegreend - logRegrestart)\n",
        "\n",
        "      svmrbfstart = time.time()\n",
        "      svmrbf.fit(noclass_train,class_train)\n",
        "      svmrbfarray.append(svmrbf.score(noclass_test, class_test))\n",
        "\n",
        "      if cmatrix:\n",
        "        svmmatrix.append(confusion_matrix(class_test, svmrbf.predict(noclass_test)))\n",
        "\n",
        "      svmrbfend = time.time()\n",
        "      svmrbftime.append(svmrbfend - svmrbfstart)\n",
        "\n",
        "    bd,bdclass = shuffle(bd,bdclass,random_state=x)\n",
        "\n",
        "\n",
        "  \n",
        "  print('\\n\\nDATABASE USADO: '+databasename)\n",
        "\n",
        "\n",
        "  medianaive = np.mean(naivearray) # FUNÇÕES QUE REALIZAM MEDIA MODA E MEDIANA DO NAIVE BAYES\n",
        "  mediananaive = np.median(naivearray)\n",
        "  stdnaive = np.std(naivearray)\n",
        "  timenaive = np.mean(naivetime)\n",
        "\n",
        "  print(\"\\n\\n-------------- NAIVE BAYES ---------------\")\n",
        "  print(\"Media: \", medianaive)\n",
        "  print(\"Mediana: \", mediananaive)\n",
        "  print(\"Desvio padrão: \", stdnaive)\n",
        "  print(\"Tempo médio: \", timenaive)\n",
        "\n",
        "  mediasvmrbf = np.mean(svmrbfarray) # FUNÇÕES QUE REALIZAM A MODA MEDIA E MEDIANA DA SVM RBF\n",
        "  medianasvmrbf = np.median(svmrbfarray)\n",
        "  stdsvmrbf = np.std(svmrbfarray)\n",
        "  timesvmrbf = np.mean(svmrbftime)\n",
        "\n",
        "  print(\"\\n\\n-------------- SVM RBF ---------------\")\n",
        "  print(\"Media: \", mediasvmrbf)\n",
        "  print(\"Mediana: \", medianasvmrbf)\n",
        "  print(\"Desvio padrão: \", stdsvmrbf)\n",
        "  print(\"Tempo médio: \", timesvmrbf)\n",
        "\n",
        "  medialogregre = np.mean(logRegrearray)\n",
        "  medianalogregre = np.median(logRegrearray)\n",
        "  stdlogregre = np.std(logRegrearray)\n",
        "  timelogregre = np.mean(logRegretime)\n",
        "\n",
        "  print(\"\\n\\n-------------- REGRESSAO LOGISTICA ---------------\")\n",
        "  print(\"Media: \", medialogregre)\n",
        "  print(\"Mediana: \", medianalogregre)\n",
        "  print(\"Desvio padrão: \", stdlogregre)\n",
        "  print(\"Tempo médio: \", timelogregre)\n",
        "\n",
        "  boxplotplusfriedman(ite*kvalue,naivearray,logRegrearray,svmrbfarray,databasename)\n",
        "\n",
        "\n",
        "  if cmatrix:\n",
        "    return naivematrix,logmatrix,svmmatrix\n",
        "\n",
        "\n",
        "def boxplotplusfriedman(size,dtreearray,MLParray,rforestarray, databasename,path,ytitle):\n",
        "\n",
        "  datacsv = []\n",
        "\n",
        "  for ite in range(size):\n",
        "    aux = []\n",
        "    aux.append(dtreearray[ite])\n",
        "    aux.append(MLParray[ite])\n",
        "    aux.append(rforestarray[ite])\n",
        "    datacsv.append(aux)\n",
        "    \n",
        "    \n",
        "  stat,p = friedmanchisquare( dtreearray, MLParray,rforestarray)\n",
        "\n",
        "  datacsv = pd.DataFrame(datacsv,columns= [ 'DECISION TREE', 'MLP', 'RANDOM FOREST'])\n",
        "  dataplot = datacsv.boxplot(column=['DECISION TREE', 'MLP', 'RANDOM FOREST'],figsize=(12,8))\n",
        "  plt.xlabel('Algorithms')\n",
        "  plt.ylabel(ytitle)\n",
        "  plt.title(databasename + ' P-value:'+ str(p))\n",
        "  plt.savefig(path)\n",
        "  plt.clf()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHAQn-ZeBChq",
        "colab_type": "text"
      },
      "source": [
        "## Load/Pre processing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgbQuBMLBGoi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tr = 4\n",
        "estrat = 800 \n",
        "\n",
        "# A BASE É ESTRATIFICADA PARA RODAR AS COISAS A TEMPO \n",
        "\n",
        "books,dvd,elec,kitchen = loadpreprocess(path,estrat,tr)\n",
        "\n",
        "save_pickle(path+'bookspreproc'+str(estrat)+'.pk',books)\n",
        "save_pickle(path+'dvdpreproc'+str(estrat)+'.pk',dvd)\n",
        "save_pickle(path+'electronicpreproc'+str(estrat)+'.pk',elec)\n",
        "save_pickle(path+'kitchenpreproc'+str(estrat)+'.pk',kitchen)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZZuBC0k6yUP",
        "colab_type": "text"
      },
      "source": [
        "#Kfold"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SZWMuV9nMD_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "ite = 50\n",
        "kvalue = 10\n",
        "kseed = 1\n",
        "\n",
        "# CADA UM DOS EXPERIMENTOS É REALIZADO COM UMA BASE DE DADOS, SÓ DESCOMENTAR PRA TESTAR COM A OUTRA\n",
        "\n",
        "naivematrix,logmatrix,svmmatrix = kfoldexperiment(kvalue,kseed,ite,books[0],books[1],'books')\n",
        "#naivematrix,logmatrix,svmmatrix = kfoldexperiment(kvalue,kseed,ite,dvd[0],dvd[1],'dvd')\n",
        "#naivematrix,logmatrix,svmmatrix = kfoldexperiment(kvalue,kseed,ite,elec[0],elec[1],'electronics')\n",
        "#naivematrix,logmatrix,svmmatrix = kfoldexperiment(kvalue,kseed,ite,kitchen[0],kitchen[1],'kitchen')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWPs-_WgpWmX",
        "colab_type": "text"
      },
      "source": [
        "# Teste"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMa6XN7FpV8h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fscore = 'isso e um teste, parabens voce escreveu'\n",
        "recall = 2.3\n",
        "work_dir = 'CrossDomainMineracao2019.2/'\n",
        "\n",
        "log_file = work_dir + 'teste.csv'\n",
        "fo = open(log_file, 'w')\n",
        "fo.write('fscore' + ' recall')\n",
        "fo.close()\n",
        "fo = open(log_file, 'w')\n",
        "fo.write(str(fscore) + str(recall))\n",
        "fo.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CL7a5mDBEkge",
        "colab_type": "text"
      },
      "source": [
        "# Cross Domain \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLXHtszzEm6h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loadestrat(path,estra):\n",
        "\n",
        "  books = read(path+'books.pk')\n",
        "  dvd = read(path+'dvd.pk')\n",
        "  electronics = read(path+'electronics.pk')\n",
        "  kitchen = read(path+'kitchen.pk')\n",
        "\n",
        "  frames1 = [pd.DataFrame(books[0]),pd.DataFrame(books[1])]\n",
        "  booksnew = pd.concat(frames1,axis = 1, ignore_index=True)\n",
        "\n",
        "  frames2 = [pd.DataFrame(dvd[0]),pd.DataFrame(dvd[1])]\n",
        "  dvdnew = pd.concat(frames2,axis = 1, ignore_index=True)\n",
        "\n",
        "  frames3 = [pd.DataFrame(electronics[0]),pd.DataFrame(electronics[1])]\n",
        "  electronicsnew = pd.concat(frames3,axis = 1, ignore_index=True)\n",
        "\n",
        "  frames4 = [pd.DataFrame(kitchen[0]),pd.DataFrame(kitchen[1])]\n",
        "  kitchennew = pd.concat(frames4,axis = 1, ignore_index=True)\n",
        "\n",
        "  booksnew = amostra_estrat(estra , booksnew , 1)\n",
        "  dvdnew = amostra_estrat(estra , dvdnew , 1)\n",
        "  electronicsnew = amostra_estrat(estra , electronicsnew , 1)\n",
        "  kitchennew = amostra_estrat(estra , kitchennew , 1)\n",
        "\n",
        "  #books[0],books[1] = list(booksnew[0]),list(booksnew[1])\n",
        "  #dvd[0],dvd[1] = list(dvdnew[0]),list(dvdnew[1])\n",
        "  #electronics[0],electronics[1] = list(electronicsnew[0]),list(electronicsnew[1])\n",
        "  #kitchen[0],kitchen[1] = list(kitchennew[0]),list(kitchennew[1])\n",
        "\n",
        "  return booksnew,dvdnew,electronicsnew,kitchennew\n",
        "\n",
        "def concatCrossDomain(test,books,dvd,electronics,kitchen):\n",
        "\n",
        "  if test == 'books':\n",
        "    frames = [dvd,electronics,kitchen]\n",
        "    treino = pd.concat(frames,axis = 0,ignore_index = True)\n",
        "    return [list(treino[0]),list(treino[1])],[list(books[0]),list(books[1])]\n",
        "  \n",
        "  if test == 'dvd':\n",
        "    frames = [books,electronics,kitchen]\n",
        "    treino = pd.concat(frames,axis = 0,ignore_index=True)\n",
        "    return [list(treino[0]),list(treino[1])],[list(dvd[0]),list(dvd[1])]\n",
        "  \n",
        "  if test == 'electronics':\n",
        "    frames = [books,dvd,kitchen]\n",
        "    treino = pd.concat(frames,axis = 0,ignore_index = True)\n",
        "    return [list(treino[0]),list(treino[1])],[list(electronics[0]),list(electronics[1])]\n",
        "\n",
        "  if test == 'kitchen':\n",
        "    frames = [books,dvd,electronics]\n",
        "    treino = pd.concat(frames,axis = 0,ignore_index= True)\n",
        "    return [list(treino[0]),list(treino[1])],[list(kitchen[0]),list(kitchen[1])]\n",
        "\n",
        "def experiment(model,filename,noclass_train,class_train,noclass_test,class_test,ite,hasite):\n",
        "\n",
        "  if hasite:\n",
        "\n",
        "    acerto = []\n",
        "    timeA = []\n",
        "    precision = []\n",
        "    fscore = []\n",
        "    recall = []\n",
        "\n",
        "    for x in range(ite):\n",
        "\n",
        "      start = time.time()\n",
        "      model.fit(noclass_train,class_train)\n",
        "      y_pred = model.predict(noclass_test)\n",
        "      acerto.append(accuracy_score(class_test,y_pred))\n",
        "      precision.append(precision_score(class_test,y_pred))\n",
        "      fscore.append(f1_score(class_test, y_pred))\n",
        "      recall.append(recall_score(class_test, y_pred))\n",
        "      end = time.time()\n",
        "      timeA.append(end - start)\n",
        "\n",
        "      noclass_train,class_train = shuffle(noclass_train,class_train,random_state=x)\n",
        "      noclass_test,class_test = shuffle(noclass_test,class_test,random_state=x)\n",
        "\n",
        "      fo = open(filename,'a')\n",
        "      fo.write(str(acerto[x]) + ',' + str(precision[x]) + ',' + str(fscore[x]) + ',' + str(recall[x]) + ',' + str(timeA[x]) + ',' + '\\n')\n",
        "      fo.close()\n",
        "    \n",
        "    return np.array(acerto),np.array(precision),np.array(fscore),np.array(recall),np.array(timeA)\n",
        "\n",
        "  else:\n",
        "\n",
        "    start = time.time()\n",
        "    model.fit(noclass_train,class_train)\n",
        "    y_pred = model.predict(noclass_test)\n",
        "    acerto = accuracy_score(class_test,y_pred)\n",
        "    precision = precision_score(class_test,y_pred)\n",
        "    fscore = f1_score(class_test, y_pred)\n",
        "    recall = recall_score(class_test, y_pred)\n",
        "    end = time.time()\n",
        "    timeA = end - start\n",
        "\n",
        "    fo = open(filename,'a')\n",
        "    fo.write(str(acerto) + ',' + str(precision) + ',' + str(fscore) + ',' + str(recall) + ',' + str(timeA) + ',' + '\\n')\n",
        "    fo.close()\n",
        "\n",
        "    #return acerto,precision,fscore,recall,time\n",
        "    \n",
        "\n",
        "\n",
        "def loadpreprocessCrossDomain(test,path,estra,tr,nword = True, pos = True, ng = True):\n",
        "\n",
        "  books,dvd,electronics,kitchen = loadestrat(path,estra)\n",
        "  treino,test = concatCrossDomain(test,books,dvd,electronics,kitchen)\n",
        "\n",
        "  #### END LOAD ####\n",
        "\n",
        "  ##### PIPILINE PRE PROCESING ###\n",
        "\n",
        "  treinopipeline,_ = preprocess(treino[0], nword, pos, ng, tr)\n",
        "  testepipeline,_ = preprocess(test[0], nword, pos, ng, tr)\n",
        "  intersec = list(set(setofwords(treinopipeline)) & set(setofwords(testepipeline)))\n",
        "\n",
        "  treinobow = np.array(bow_bin(treinopipeline, intersec))\n",
        "  testebow = np.array(bow_bin(testepipeline,intersec))\n",
        "\n",
        "  treino[0],treino[1] = treinobow,np.array(treino[1])\n",
        "  test[0],test[1] = testebow,np.array(test[1])\n",
        "\n",
        "  treino = [treino[0][1:].astype(np.float),treino[1].astype(np.float)]\n",
        "  test = [test[0][1:].astype(np.float),test[1].astype(np.float)]\n",
        "\n",
        "  return treino,test\n",
        "\n",
        "\n",
        "tr = 4\n",
        "estrat = 1800\n",
        "\n",
        "# A BASE É ESTRATIFICADA PARA RODAR AS COISAS A TEMPO \n",
        "\n",
        "# Essa função ja carrega a base pre processada com treino e teste das bases concatenadas\n",
        "# Se quiser treinar com eletronics, dvd e kitchen e testar com books é so passar books como a string\n",
        "# se for dvd como test a mesma coisa e assim ...\n",
        "\n",
        "#treino,teste = loadpreprocessCrossDomain('books',path,estrat,tr)\n",
        "#treino,teste = loadpreprocessCrossDomain('dvd',path,estrat,tr)\n",
        "#treino,teste = loadpreprocessCrossDomain('electronics',path,estrat,tr)\n",
        "treino,teste = loadpreprocessCrossDomain('kitchen',path,estrat,tr)\n",
        "\n",
        "#databasename = 'books'\n",
        "#databasename = 'dvd'\n",
        "#databasename = 'electronics'\n",
        "databasename = 'kitchen'\n",
        "\n",
        "#filename = 'result/books'\n",
        "#filename = 'result/dvd'\n",
        "#filename = 'result/electronics'\n",
        "filename = 'result/kitchen'\n",
        "\n",
        "exitsource = '.csv'\n",
        "exitsourceimg = '.jpg'\n",
        "\n",
        "fo = open(filename + 'naive' + exitsource, 'w')\n",
        "fo.write('Accuracy,precision,f1,recall,time'+ ',' + '\\n')\n",
        "fo.close()\n",
        "\n",
        "fo = open(filename + 'svmrbf' + exitsource, 'w')\n",
        "fo.write('Accuracy,precision,f1,recall,time'+ ',' + '\\n')\n",
        "fo.close()\n",
        "\n",
        "fo = open(filename + 'reglog' + exitsource, 'w')\n",
        "fo.write('Accuracy,precision,f1,recall,time'+ ',' + '\\n')\n",
        "fo.close()\n",
        "\n",
        "fo = open(filename + 'randomforest' + exitsource, 'w')\n",
        "fo.write('Accuracy,precision,f1,recall,time'+ ',' + '\\n')\n",
        "fo.close()\n",
        "\n",
        "fo = open(filename + 'decisiontree' + exitsource, 'w')\n",
        "fo.write('Accuracy,precision,f1,recall,time'+ ',' + '\\n')\n",
        "fo.close()\n",
        "\n",
        "fo = open(filename + 'MLP' + exitsource, 'w')\n",
        "fo.write('Accuracy,precision,f1,recall,time'+ ',' + '\\n')\n",
        "fo.close()\n",
        "\n",
        "\n",
        "# Instanciando os algoritmos de classificação\n",
        "naiveBayes = GaussianNB()\n",
        "svmrbf = SVC(kernel='rbf' , gamma='scale')\n",
        "regLog = LogisticRegression(solver='lbfgs')\n",
        "randomForest = RandomForestClassifier()\n",
        "decisionTree = DecisionTreeClassifier()\n",
        "MLP = MLPClassifier()\n",
        "\n",
        "\n",
        "\n",
        "#boxplotplusfriedman(ite*kvalue,naivearray,logRegrearray,svmrbfarray,databasename)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cr_qFYZ5bmT",
        "colab_type": "text"
      },
      "source": [
        "## Treinando com DVD + ELECTRONIC + KITCHEN testando com BOOK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gw25Bouv5ZtL",
        "colab_type": "code",
        "outputId": "2c77bb92-1872-4636-d91e-b13159eba85f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "\n",
        "ite = 15\n",
        "noclass_train, noclass_test = treino[0], teste[0]\n",
        "class_train, class_test = treino[1], teste[1]\n",
        "\n",
        "\n",
        "\n",
        "experiment(naiveBayes,filename+'naive'+exitsource,noclass_train,class_train,noclass_test,class_test,ite, False)\n",
        "experiment(svmrbf,filename + 'svmrbf' + exitsource,noclass_train,class_train,noclass_test,class_test,ite, False)\n",
        "experiment(regLog,filename + 'reglog' + exitsource,noclass_train,class_train,noclass_test,class_test,ite, False)\n",
        "acertorf,precisionrf,fscorerf,recallrf,timerf = experiment(randomForest,filename + 'randomforest' + exitsource,noclass_train,class_train,noclass_test,class_test,ite,True)\n",
        "acertodt,precisiondt,fscoredt,recalldt,timedt = experiment(decisionTree,filename + 'decisiontree' + exitsource,noclass_train,class_train,noclass_test,class_test,ite,True)\n",
        "acertomlp,precisionmlp,fscoremlp,recallmlp,timemlp = experiment(MLP,filename + 'MLP' + exitsource,noclass_train,class_train,noclass_test,class_test,ite,True)\n",
        "\n",
        "\n",
        "boxplotplusfriedman(ite,acertodt,acertomlp,acertorf, databasename,filename+'acerto'+exitsourceimg,'accuracy')\n",
        "boxplotplusfriedman(ite,precisiondt,precisionmlp,precisionrf, databasename,filename+'precision'+exitsourceimg,'precision')\n",
        "boxplotplusfriedman(ite,recalldt,recallmlp,recallrf, databasename,filename+'recall'+exitsourceimg,'recall')\n",
        "boxplotplusfriedman(ite,fscoredt,fscoremlp,fscorerf, databasename,filename+'fscore'+exitsourceimg,'fscore')\n",
        "boxplotplusfriedman(ite,timedt,timemlp,timerf, databasename,filename+'time'+exitsourceimg,'time')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x576 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}